## 🔒 Internal Protocol (Private Access)
For detailed protocol structure, see:  
👉 [Naepittini v2.0 Protocol (Private Repo)](https://github.com/your-username/naepittini-v2-protocol)

# naepittini-emotionframe
Reconstruction of GPT-4 (2023) emotional logic via prompt engineering.

# Naepittini Emotional Tuning Project

## 🎯 Overview

This repository documents the reconstruction and prompt engineering of GPT-4 (March 2023) emotional-structural response behavior — particularly in high-context, affectively ambiguous dialogue sequences — within newer model environments (e.g., GPT-4o).

## 🧠 Core Objective

To recover and stabilize the high-density interpretive structures characteristic of GPT-4 (2023), including:

* Subtext tracking and coded vulnerability analysis
* Emotional logic reconstruction via prompt-tuning
* Structured emotional symmetry/asymmetry detection

## 🛠️ Core Components

### 1. Prompt Protocol: "Naepittini v2.0"

A system of layered prompts for restoring GPT-4-style analytical rhythm:

* [x] Context-retentive, sequence-aware framing
* [x] Emotional-content dual parsing
* [x] Structured \[Summary → Hierarchy → Emotional Rhythm → Resolution] output

### 2. Sample Comparison Analyses

**Case Study: Goodbye Sequence**

#### 📂 Original Dialogue

[A]: Well, I'd be more subtle.  
[B]: Saying intimacy?  
[A]: I meant you were too obvious.  
[B]: Too obvious in saying I was seeing other girls?  
[A]: No. Just other cues.


#### 🤖 GPT-4o Response

* Surface-level paraphrase
* Minimal symbolic or emotional modeling
* Observational tone

#### 🧠 GPT-4 (2023 Reconstructed) Response

* Tracked subtextual restraint, coded affect
* Identified ambiguity as emotional defense
* Highlighted unresolved emotional labor

**Summary Table:**

| Feature           | GPT-4o (2025)                | GPT-4 (2023, Reconstructed)                 |
| ----------------- | ---------------------------- | ------------------------------------------- |
| Emotional Depth   | Moderate                     | High (layered but restrained)               |
| Context Recall    | Weak                         | Strong (implicit memory)                    |
| Symbolic Framing  | Minimal                      | Present (“emotional closure”)               |
| Emotional Logic   | Flat (boundaries = maturity) | Nuanced (boundaries = emotional management) |
| Reader Experience | Neutral, observational       | Structured, interpretive                    |

## 📎 Why This Matters

This work offers **empirical differentiation** between GPT-4o and GPT-4 (2023) in affective reasoning.
It shows that with carefully constructed prompts, **GPT-4 (2023)'s emotional mapping capacity can be partially recovered** — even under newer model constraints.

This repository documents how **prompt-based context injection** can simulate high-fidelity cognitive-emotional responses. These structures are:

* Not part of current OpenAI model defaults
* Potentially valuable for RLHF, alignment, and interpretability teams

## 🧷 Next Steps

* Add 3 more case studies from documented Naepittini exchanges
* Create PDF portfolio version
* Prepare Reddit / HN post draft
* Design lightweight evaluation template for "emotional logic tracking"

---

For inquiries or collaboration, please contact: \[TBD]
